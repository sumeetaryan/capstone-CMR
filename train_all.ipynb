{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "religious-strap",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we explain training steps. \n",
    "\n",
    "Here are general steps you need to follow:\n",
    "1. Python. Make sure you have a python environment (conda or other) with libraries listed in requirements.txt\n",
    "2. Data. The data preparation steps are provided in the DATA directory. Follow the prepare_data.ipynb notebook before continuing here. \n",
    "\n",
    "3. Commands. The rest of this notebook generates shell commands for starting the training process. We are running our code on a server with slurm for job management. You can change a little bit this notebook to adapt to your own environment. Providing GPU would make the running of these training jobs faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-serbia",
   "metadata": {},
   "source": [
    "## Set the params\n",
    "Define the desired settings and args..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "monetary-ticket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- experiment name: train_all_equal\n",
      "\t - data augmentation methods: full_aug\n",
      "\t - data sampling method: equal\n",
      "\t - reading data: DATA/\n",
      "\t - writing evaluations: EVAL/\n",
      "-Working on below market pairs (target, augmenting with market):\n",
      "\t--> (jp, in)\n",
      "\t--> (jp, de)\n",
      "\t--> (jp, fr)\n",
      "\t--> (jp, ca)\n",
      "\t--> (jp, mx)\n",
      "\t--> (jp, uk)\n",
      "\t--> (jp, us)\n",
      "\t--> (in, jp)\n",
      "\t--> (in, de)\n",
      "\t--> (in, fr)\n",
      "\t--> (in, ca)\n",
      "\t--> (in, mx)\n",
      "\t--> (in, uk)\n",
      "\t--> (in, us)\n",
      "\t--> (de, jp)\n",
      "\t--> (de, in)\n",
      "\t--> (de, fr)\n",
      "\t--> (de, ca)\n",
      "\t--> (de, mx)\n",
      "\t--> (de, uk)\n",
      "\t--> (de, us)\n",
      "\t--> (fr, jp)\n",
      "\t--> (fr, in)\n",
      "\t--> (fr, de)\n",
      "\t--> (fr, ca)\n",
      "\t--> (fr, mx)\n",
      "\t--> (fr, uk)\n",
      "\t--> (fr, us)\n",
      "\t--> (ca, jp)\n",
      "\t--> (ca, in)\n",
      "\t--> (ca, de)\n",
      "\t--> (ca, fr)\n",
      "\t--> (ca, mx)\n",
      "\t--> (ca, uk)\n",
      "\t--> (ca, us)\n",
      "\t--> (mx, jp)\n",
      "\t--> (mx, in)\n",
      "\t--> (mx, de)\n",
      "\t--> (mx, fr)\n",
      "\t--> (mx, ca)\n",
      "\t--> (mx, uk)\n",
      "\t--> (mx, us)\n",
      "\t--> (uk, jp)\n",
      "\t--> (uk, in)\n",
      "\t--> (uk, de)\n",
      "\t--> (uk, fr)\n",
      "\t--> (uk, ca)\n",
      "\t--> (uk, mx)\n",
      "\t--> (uk, us)\n",
      "\n",
      "-Sampling below training data fractions:\n",
      "\t--> (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# ##########\n",
    "# General Args\n",
    "# ##########\n",
    "exp_name = 'train_all'\n",
    "data_augment_method = 'full_aug' # ['no_aug', 'full_aug', 'sel_aug'] \n",
    "sampling_method = 'equal' #'concat' or 'equal'\n",
    "\n",
    "exp_name_com = f'{exp_name}_{sampling_method}' \n",
    "\n",
    "cur_data_path = 'DATA/' #'../DATA_FINAL_ELEC/'\n",
    "exp_output_dir = 'EVAL/'\n",
    "\n",
    "# $ which python\n",
    "conda_python_dir = '~/anaconda3/envs/forec/bin/python'\n",
    "\n",
    "print(f'\\n- experiment name: {exp_name_com}')\n",
    "print(f'\\t - data augmentation methods: {data_augment_method}')\n",
    "print(f'\\t - data sampling method: {sampling_method}')\n",
    "print(f'\\t - reading data: {cur_data_path}')\n",
    "print(f'\\t - writing evaluations: {exp_output_dir}')\n",
    "\n",
    "\n",
    "# ##########\n",
    "# Market selection\n",
    "# all_markets = [ 'jp', 'in', 'de', 'fr', 'ca', 'mx', 'uk', 'us'] \n",
    "# ##########\n",
    "# target_markets = ['de', 'fr']\n",
    "# source_markets = ['uk'] #, 'us' for no_aug use 'xx'\n",
    "\n",
    "target_markets = ['jp', 'in', 'de', 'fr', 'ca', 'mx', 'uk']\n",
    "source_markets = ['jp', 'in', 'de', 'fr', 'ca', 'mx', 'uk', 'us'] #, 'us' for no_aug use 'xx'\n",
    "\n",
    "\n",
    "print(f'-Working on below market pairs (target, augmenting with market):')\n",
    "all_poss_pairs = []\n",
    "for target_market in target_markets:\n",
    "    for source_market in source_markets:\n",
    "        if target_market==source_market:\n",
    "            continue\n",
    "        if data_augment_method=='no_aug':\n",
    "            source_market='xx'\n",
    "        all_poss_pairs.append((target_market, source_market))\n",
    "        print(f'\\t--> ({target_market}, {source_market})')\n",
    "all_poss_pairs = list(set(all_poss_pairs))\n",
    "\n",
    "# ##########\n",
    "# Training Data fractions to use from each target and source \n",
    "# 1 means full data, and 2 means 1/2 of the training data to sample\n",
    "# ##########\n",
    "tgt_fractions = [1]\n",
    "src_fractions = [1] #2, 3, 4, 5, 10\n",
    "\n",
    "fractions = []\n",
    "print('\\n-Sampling below training data fractions:')\n",
    "for tgt_fraction in tgt_fractions:\n",
    "    for src_fraction in src_fractions:\n",
    "        fractions.append((src_fraction, tgt_fraction))\n",
    "        print(f'\\t--> ({src_fraction}, {tgt_fraction})')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-classic",
   "metadata": {},
   "source": [
    "## Generate training shell commands\n",
    "Before being able to run MAML and FOREC models, we need to train corrosponding NMF++ models (and for NMF, we need GMF++ and MLP++ models trained first). For this purpose,\n",
    "- 'train_base.py' trains and evaluates GMF++, MLP++, and NMF++ models (if no_aug, falls into GMF, MLP, and NMF)\n",
    "- 'train_maml.py' for cross-market scenarios, trains a NMF++ model with two markets using MAML \n",
    "- 'train_forec.py' for cross-market scenarios, trains a FOREC model with two marketheads for the two markets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "sporting-success",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 49 experiments:\n",
      "train_all_equal_in_jp_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_fr_de_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_uk_fr_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_de_ca_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_mx_us_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_ca_us_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_ca_uk_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_in_us_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_jp_de_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_uk_jp_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_mx_fr_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_mx_de_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_uk_ca_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_jp_fr_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_fr_jp_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_mx_ca_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_uk_mx_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_jp_ca_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_mx_jp_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_uk_in_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_in_fr_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_ca_mx_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_ca_jp_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_in_uk_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_uk_us_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_de_mx_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_ca_in_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_fr_us_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_jp_uk_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_mx_uk_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_de_jp_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_in_de_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_de_in_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_fr_in_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_in_ca_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_ca_fr_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_fr_ca_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_fr_mx_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_jp_us_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_de_us_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_jp_in_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_mx_in_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_jp_mx_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_uk_de_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_de_fr_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_de_uk_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_fr_uk_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_in_mx_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n",
      "train_all_equal_ca_de_full_aug_ftgt1_fsrc1\n",
      "\t['base', 'maml', 'forec']\n"
     ]
    }
   ],
   "source": [
    "command_dict = {}\n",
    "for tgt_market, src_market in all_poss_pairs:\n",
    "    for tgt_frac, src_fra in fractions:\n",
    "        cur_cmd_dict = {}\n",
    "        cur_exp_name = f'{exp_name_com}_{tgt_market}_{src_market}_{data_augment_method}_ftgt{tgt_frac}_fsrc{src_fra}'\n",
    "        \n",
    "        # 'train_base.py'\n",
    "        py_file_main = 'train_base.py'\n",
    "        cur_exp_out_file = f'{exp_output_dir}base-{cur_exp_name}.json'\n",
    "        pre_set_args = {\n",
    "            \"--data_dir %s\"%(cur_data_path),\n",
    "            \"--tgt_market %s\"%(tgt_market),\n",
    "            \"--aug_src_market %s\"%(src_market),\n",
    "            \"--exp_name %s\"%(cur_exp_name),\n",
    "            \"--exp_output %s\"%(cur_exp_out_file),\n",
    "\n",
    "            \"--num_epoch %i\"%(25),  \n",
    "            \"--batch_size %i\"%(1024),\n",
    "            \"--cuda \"\n",
    "\n",
    "            \"--data_augment_method %s\"%(data_augment_method),\n",
    "            \"--data_sampling_method %s\"%(sampling_method),\n",
    "\n",
    "            \"--tgt_fraction %i\"%(tgt_frac),  \n",
    "            \"--src_fraction %i\"%(src_fra),  \n",
    "        }\n",
    "        myargumets = ' '.join(pre_set_args)\n",
    "        command_pieces = [conda_python_dir, py_file_main, myargumets]\n",
    "        final_cmd = ' '.join(command_pieces)\n",
    "        cur_cmd_dict['base'] = final_cmd\n",
    "        \n",
    "        if data_augment_method=='no_aug':\n",
    "            command_dict[cur_exp_name] = cur_cmd_dict\n",
    "            continue\n",
    "        \n",
    "        # 'train_maml.py'\n",
    "        py_file_main = 'train_maml.py'\n",
    "        fast_lr_tune = '0.1'\n",
    "        shots = 20 #512, 200, 100, 50, 20\n",
    "        cur_exp_out_file = f'{exp_output_dir}maml-{cur_exp_name}_shots{shots}.json'\n",
    "        pre_set_args = {\n",
    "            \"--data_dir %s\"%(cur_data_path),\n",
    "            \"--tgt_market %s\"%(tgt_market),\n",
    "            \"--aug_src_market %s\"%(src_market),\n",
    "            \"--exp_name %s\"%(cur_exp_name),\n",
    "            \"--exp_output %s\"%(cur_exp_out_file),\n",
    "\n",
    "            \"--num_epoch %i\"%(25),  \n",
    "            \"--batch_size %i\"%(shots),\n",
    "            \"--cuda \"\n",
    "\n",
    "            \"--data_sampling_method %s\"%(sampling_method),\n",
    "            \"--fast_lr %s\"%(fast_lr_tune),\n",
    "            \"--tgt_fraction %i\"%(tgt_frac),  \n",
    "            \"--src_fraction %i\"%(src_fra),  \n",
    "        }\n",
    "        myargumets = ' '.join(pre_set_args)\n",
    "        command_pieces = [conda_python_dir, py_file_main, myargumets]\n",
    "        final_cmd = ' '.join(command_pieces)\n",
    "        cur_cmd_dict['maml'] = final_cmd\n",
    "        \n",
    "        # 'train_forec.py'\n",
    "        py_file_main = 'train_forec.py'\n",
    "        cur_exp_out_file = f'{exp_output_dir}forec-{cur_exp_name}_shots{shots}.json'\n",
    "        pre_set_args = {\n",
    "            \"--data_dir %s\"%(cur_data_path),\n",
    "            \"--tgt_market %s\"%(tgt_market),\n",
    "            \"--aug_src_market %s\"%(src_market),\n",
    "            \"--exp_name %s\"%(cur_exp_name),\n",
    "            \"--exp_output %s\"%(cur_exp_out_file),\n",
    "\n",
    "            \"--num_epoch %i\"%(25),  \n",
    "            \"--batch_size %i\"%(shots),\n",
    "            \"--cuda \"\n",
    "\n",
    "            \"--data_sampling_method %s\"%(sampling_method),\n",
    "            \"--fast_lr %s\"%(fast_lr_tune),\n",
    "            \"--tgt_fraction %i\"%(tgt_frac),  \n",
    "            \"--src_fraction %i\"%(src_fra),  \n",
    "        }\n",
    "        myargumets = ' '.join(pre_set_args)\n",
    "        command_pieces = [conda_python_dir, py_file_main, myargumets]\n",
    "        final_cmd = ' '.join(command_pieces)\n",
    "        cur_cmd_dict['forec'] = final_cmd\n",
    "        \n",
    "        command_dict[cur_exp_name] = cur_cmd_dict\n",
    "        \n",
    "print(f'Generated {len(command_dict)} experiments:')\n",
    "for k, v in command_dict.items():\n",
    "    print(f'{k}')\n",
    "    print(f'\\t{list(v.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-nicholas",
   "metadata": {},
   "source": [
    "## Write commands into .sh bash scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "encouraging-consumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_all_equal_in_jp_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_fr_de_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_uk_fr_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_de_ca_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_mx_us_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_ca_us_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_ca_uk_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_in_us_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_jp_de_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_uk_jp_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_mx_fr_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_mx_de_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_uk_ca_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_jp_fr_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_fr_jp_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_mx_ca_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_uk_mx_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_jp_ca_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_mx_jp_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_uk_in_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_in_fr_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_ca_mx_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_ca_jp_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_in_uk_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_uk_us_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_de_mx_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_ca_in_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_fr_us_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_jp_uk_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_mx_uk_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_de_jp_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_in_de_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_de_in_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_fr_in_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_in_ca_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_ca_fr_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_fr_ca_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_fr_mx_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_jp_us_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_de_us_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_jp_in_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_mx_in_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_jp_mx_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_uk_de_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_de_fr_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_de_uk_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_fr_uk_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_in_mx_full_aug_ftgt1_fsrc1 bash is created!\n",
      "train_all_equal_ca_de_full_aug_ftgt1_fsrc1 bash is created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "sh_files = 'scripts/'\n",
    "sh_logs = os.path.join(sh_files,'logs')\n",
    "checkpoints_dir = 'checkpoints/'\n",
    "if not os.path.exists(sh_logs):\n",
    "    os.mkdir(sh_files)\n",
    "    os.mkdir(sh_logs)\n",
    "if not os.path.exists(exp_output_dir):\n",
    "    os.mkdir(exp_output_dir)\n",
    "if not os.path.exists(checkpoints_dir):\n",
    "    os.mkdir(checkpoints_dir)\n",
    "\n",
    "gpu_num = 1\n",
    "gpu_type = '1080ti-long' #'titanx-short', 'm40-short'\n",
    "\n",
    "master_file = open(os.path.join(sh_files,'master.sh'), 'w')\n",
    "\n",
    "for cur_exp_name, v in command_dict.items():\n",
    "\n",
    "    bash_file_name = f'{cur_exp_name}-run.sh'\n",
    "    bash_file = open(os.path.join(sh_files,bash_file_name), 'w')\n",
    "    cur_log_file = os.path.join('logs', f'{cur_exp_name}.out')\n",
    "    \n",
    "    bash_file.write('#!/bin/sh'+'\\n')\n",
    "    bash_file.write('#SBATCH --partition=%s'%(gpu_type) + '\\n')\n",
    "    bash_file.write('#SBATCH --ntasks=%s'%(1) + '\\n')\n",
    "    bash_file.write('#SBATCH --gres=gpu:%s'%(str(gpu_num)) + '\\n')\n",
    "    bash_file.write('#SBATCH --mem=%iG'%(50*gpu_num) + '\\n')\n",
    "    bash_file.write('#SBATCH --output=%s'%(cur_log_file) + '\\n')\n",
    "\n",
    "    bash_file.write('\\ncd ..\\n')\n",
    "    if 'base' in v:\n",
    "        bash_file.write(v['base'] + '\\n\\n')\n",
    "    if 'maml' in v:\n",
    "        bash_file.write(v['maml'] + '\\n\\n')\n",
    "    if 'forec' in v:\n",
    "        bash_file.write(v['forec'] + '\\n\\n')\n",
    "\n",
    "    bash_file.close()\n",
    "    master_file.write(f'sbatch {bash_file_name}\\n')\n",
    "    print(cur_exp_name + ' bash is created!')\n",
    "\n",
    "master_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "solar-samuel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 8377817\n",
      "Submitted batch job 8377818\n",
      "Submitted batch job 8377819\n",
      "Submitted batch job 8377820\n",
      "Submitted batch job 8377821\n",
      "Submitted batch job 8377822\n",
      "Submitted batch job 8377823\n",
      "Submitted batch job 8377824\n",
      "Submitted batch job 8377825\n",
      "Submitted batch job 8377826\n",
      "Submitted batch job 8377827\n",
      "Submitted batch job 8377828\n",
      "Submitted batch job 8377829\n",
      "Submitted batch job 8377830\n",
      "Submitted batch job 8377831\n",
      "Submitted batch job 8377832\n",
      "Submitted batch job 8377833\n",
      "Submitted batch job 8377834\n",
      "Submitted batch job 8377835\n",
      "Submitted batch job 8377836\n",
      "Submitted batch job 8377837\n",
      "Submitted batch job 8377838\n",
      "Submitted batch job 8377839\n",
      "Submitted batch job 8377840\n",
      "Submitted batch job 8377841\n",
      "Submitted batch job 8377842\n",
      "Submitted batch job 8377843\n",
      "Submitted batch job 8377844\n",
      "Submitted batch job 8377845\n",
      "Submitted batch job 8377846\n",
      "Submitted batch job 8377847\n",
      "Submitted batch job 8377848\n",
      "Submitted batch job 8377849\n",
      "Submitted batch job 8377850\n",
      "Submitted batch job 8377851\n",
      "Submitted batch job 8377852\n",
      "Submitted batch job 8377853\n",
      "Submitted batch job 8377854\n",
      "Submitted batch job 8377855\n",
      "Submitted batch job 8377856\n",
      "Submitted batch job 8377857\n",
      "Submitted batch job 8377858\n",
      "Submitted batch job 8377859\n",
      "Submitted batch job 8377860\n",
      "Submitted batch job 8377861\n",
      "Submitted batch job 8377862\n",
      "Submitted batch job 8377863\n",
      "Submitted batch job 8377864\n",
      "Submitted batch job 8377865\n"
     ]
    }
   ],
   "source": [
    "!cd scripts/ && chmod +x *.sh && ./master.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "medium-webcam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           8377841 1080ti-lo train_al    bonab  R    8:57:51      1 node120\n",
      "           8377856 1080ti-lo train_al    bonab  R    8:57:51      1 node117\n",
      "           8376528  m40-long     bash    bonab  R 2-00:42:20      1 node013\n"
     ]
    }
   ],
   "source": [
    "!squeue -u bonab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "collect-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scancel 8377816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-catholic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_transformer3_qbd",
   "language": "python",
   "name": "py37_transformer3_qbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
