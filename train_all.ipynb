{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "religious-strap",
      "metadata": {
        "id": "religious-strap"
      },
      "source": [
        "## Introduction\n",
        "In this notebook, we explain training steps.\n",
        "\n",
        "Here are general steps you need to follow:\n",
        "1. Python. Make sure you have a python environment (conda or other) with libraries listed in requirements.txt\n",
        "2. Data. The data preparation steps are provided in the DATA directory. Follow the prepare_data.ipynb notebook before continuing here.\n",
        "\n",
        "3. Commands. The rest of this notebook generates shell commands for starting the training process. We are running our code on a server with slurm for job management. You can change a little bit this notebook to adapt to your own environment. Providing GPU would make the running of these training jobs faster."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "blessed-serbia",
      "metadata": {
        "id": "blessed-serbia"
      },
      "source": [
        "## Set the params\n",
        "Define the desired settings and args..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/requirements.txt"
      ],
      "metadata": {
        "id": "-_oDKH2PJLbE",
        "outputId": "3c715f95-3f5d-4c28-9004-8122d3a622b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-_oDKH2PJLbE",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting learn2learn==0.1.6 (from -r /content/requirements.txt (line 1))\n",
            "  Downloading learn2learn-0.1.6.tar.gz (604 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m604.9/604.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement numpy==1.21.4 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0, 1.25.1, 1.25.2, 1.26.0, 1.26.1, 1.26.2, 1.26.3, 1.26.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0rc1, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0rc1, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.2.6, 2.3.0rc1, 2.3.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for numpy==1.21.4\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/DATA2"
      ],
      "metadata": {
        "id": "bVEF0IjgGsvi",
        "outputId": "5aad4831-f23f-4978-a972-7bd0bdaacddb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bVEF0IjgGsvi",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/DATA2.zip\n",
            "   creating: DATA2/\n",
            "   creating: DATA2/proc_data/\n",
            "  inflating: DATA2/proc_data/us_5core.txt  \n",
            "  inflating: DATA2/proc_data/us_10core.txt  \n",
            "  inflating: DATA2/proc_data/jp_5core.txt  \n",
            "  inflating: DATA2/proc_data/ca_5core.txt  \n",
            "  inflating: DATA2/proc_data/uk_5core.txt  \n",
            "  inflating: DATA2/proc_data/fr_5core.txt  \n",
            "  inflating: DATA2/proc_data/de_5core.txt  \n",
            "  inflating: DATA2/proc_data/in_5core.txt  \n",
            "  inflating: DATA2/proc_data/mx_5core.txt  \n",
            "   creating: DATA2/orig_data/\n",
            "  inflating: DATA2/orig_data/ratings_uk_Electronics.txt.gz  \n",
            "  inflating: DATA2/orig_data/ratings_in_Electronics.txt.gz  \n",
            "  inflating: DATA2/orig_data/ratings_jp_Electronics.txt.gz  \n",
            "  inflating: DATA2/orig_data/ratings_mx_Electronics.txt.gz  \n",
            "  inflating: DATA2/orig_data/ratings_de_Electronics.txt.gz  \n",
            "  inflating: DATA2/orig_data/ratings_ca_Electronics.txt.gz  \n",
            "  inflating: DATA2/orig_data/ratings_fr_Electronics.txt.gz  \n",
            "  inflating: DATA2/orig_data/ratings_us_Electronics.txt.gz  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "monetary-ticket",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "monetary-ticket",
        "outputId": "2810f2d3-c56d-4bca-8031-a78d4679fc08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "- experiment name: train_all_equal\n",
            "\t - data augmentation methods: full_aug\n",
            "\t - data sampling method: equal\n",
            "\t - reading data: DATA/\n",
            "\t - writing evaluations: EVAL/\n",
            "-Working on below market pairs (target, augmenting with market):\n",
            "\t--> (jp, in)\n",
            "\t--> (jp, de)\n",
            "\t--> (jp, fr)\n",
            "\t--> (jp, ca)\n",
            "\t--> (jp, mx)\n",
            "\t--> (jp, uk)\n",
            "\t--> (jp, us)\n",
            "\t--> (in, jp)\n",
            "\t--> (in, de)\n",
            "\t--> (in, fr)\n",
            "\t--> (in, ca)\n",
            "\t--> (in, mx)\n",
            "\t--> (in, uk)\n",
            "\t--> (in, us)\n",
            "\t--> (de, jp)\n",
            "\t--> (de, in)\n",
            "\t--> (de, fr)\n",
            "\t--> (de, ca)\n",
            "\t--> (de, mx)\n",
            "\t--> (de, uk)\n",
            "\t--> (de, us)\n",
            "\t--> (fr, jp)\n",
            "\t--> (fr, in)\n",
            "\t--> (fr, de)\n",
            "\t--> (fr, ca)\n",
            "\t--> (fr, mx)\n",
            "\t--> (fr, uk)\n",
            "\t--> (fr, us)\n",
            "\t--> (ca, jp)\n",
            "\t--> (ca, in)\n",
            "\t--> (ca, de)\n",
            "\t--> (ca, fr)\n",
            "\t--> (ca, mx)\n",
            "\t--> (ca, uk)\n",
            "\t--> (ca, us)\n",
            "\t--> (mx, jp)\n",
            "\t--> (mx, in)\n",
            "\t--> (mx, de)\n",
            "\t--> (mx, fr)\n",
            "\t--> (mx, ca)\n",
            "\t--> (mx, uk)\n",
            "\t--> (mx, us)\n",
            "\t--> (uk, jp)\n",
            "\t--> (uk, in)\n",
            "\t--> (uk, de)\n",
            "\t--> (uk, fr)\n",
            "\t--> (uk, ca)\n",
            "\t--> (uk, mx)\n",
            "\t--> (uk, us)\n",
            "\n",
            "-Sampling below training data fractions:\n",
            "\t--> (1, 1)\n"
          ]
        }
      ],
      "source": [
        "# ##########\n",
        "# General Args\n",
        "# ##########\n",
        "exp_name = 'train_all'\n",
        "data_augment_method = 'full_aug' # ['no_aug', 'full_aug', 'sel_aug']\n",
        "sampling_method = 'equal' #'concat' or 'equal'\n",
        "\n",
        "exp_name_com = f'{exp_name}_{sampling_method}'\n",
        "\n",
        "cur_data_path = 'DATA/' #'../DATA_FINAL_ELEC/'\n",
        "exp_output_dir = 'EVAL/'\n",
        "\n",
        "# $ which python\n",
        "conda_python_dir = '~/anaconda3/envs/forec/bin/python'\n",
        "\n",
        "print(f'\\n- experiment name: {exp_name_com}')\n",
        "print(f'\\t - data augmentation methods: {data_augment_method}')\n",
        "print(f'\\t - data sampling method: {sampling_method}')\n",
        "print(f'\\t - reading data: {cur_data_path}')\n",
        "print(f'\\t - writing evaluations: {exp_output_dir}')\n",
        "\n",
        "\n",
        "# ##########\n",
        "# Market selection\n",
        "# all_markets = [ 'jp', 'in', 'de', 'fr', 'ca', 'mx', 'uk', 'us']\n",
        "# ##########\n",
        "# target_markets = ['de', 'fr']\n",
        "# source_markets = ['uk'] #, 'us' for no_aug use 'xx'\n",
        "\n",
        "target_markets = ['jp', 'in', 'de', 'fr', 'ca', 'mx', 'uk']\n",
        "source_markets = ['jp', 'in', 'de', 'fr', 'ca', 'mx', 'uk', 'us'] #, 'us' for no_aug use 'xx'\n",
        "\n",
        "\n",
        "print(f'-Working on below market pairs (target, augmenting with market):')\n",
        "all_poss_pairs = []\n",
        "for target_market in target_markets:\n",
        "    for source_market in source_markets:\n",
        "        if target_market==source_market:\n",
        "            continue\n",
        "        if data_augment_method=='no_aug':\n",
        "            source_market='xx'\n",
        "        all_poss_pairs.append((target_market, source_market))\n",
        "        print(f'\\t--> ({target_market}, {source_market})')\n",
        "all_poss_pairs = list(set(all_poss_pairs))\n",
        "\n",
        "# ##########\n",
        "# Training Data fractions to use from each target and source\n",
        "# 1 means full data, and 2 means 1/2 of the training data to sample\n",
        "# ##########\n",
        "tgt_fractions = [1]\n",
        "src_fractions = [1] #2, 3, 4, 5, 10\n",
        "\n",
        "fractions = []\n",
        "print('\\n-Sampling below training data fractions:')\n",
        "for tgt_fraction in tgt_fractions:\n",
        "    for src_fraction in src_fractions:\n",
        "        fractions.append((src_fraction, tgt_fraction))\n",
        "        print(f'\\t--> ({src_fraction}, {tgt_fraction})')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aboriginal-classic",
      "metadata": {
        "id": "aboriginal-classic"
      },
      "source": [
        "## Generate training shell commands\n",
        "Before being able to run MAML and FOREC models, we need to train corrosponding NMF++ models (and for NMF, we need GMF++ and MLP++ models trained first). For this purpose,\n",
        "- 'train_base.py' trains and evaluates GMF++, MLP++, and NMF++ models (if no_aug, falls into GMF, MLP, and NMF)\n",
        "- 'train_maml.py' for cross-market scenarios, trains a NMF++ model with two markets using MAML\n",
        "- 'train_forec.py' for cross-market scenarios, trains a FOREC model with two marketheads for the two markets  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "sporting-success",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sporting-success",
        "outputId": "41b1c32b-c417-49ed-de4c-1ac7ef13cd50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 49 experiments:\n",
            "train_all_equal_uk_us_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_jp_in_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_fr_jp_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_mx_in_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_ca_in_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_uk_in_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_in_ca_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_fr_uk_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_uk_de_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_de_ca_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_jp_ca_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_fr_de_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_fr_us_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_mx_ca_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_de_mx_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_in_mx_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_jp_mx_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_uk_ca_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_in_jp_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_fr_in_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_de_jp_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_de_fr_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_in_fr_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_jp_fr_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_ca_mx_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_mx_jp_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_uk_mx_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_ca_jp_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_ca_de_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_de_uk_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_in_uk_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_mx_fr_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_uk_jp_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_jp_uk_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_ca_fr_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_uk_fr_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_in_de_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_de_us_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_in_us_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_fr_ca_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_mx_uk_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_jp_us_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_jp_de_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_ca_uk_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_mx_de_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_mx_us_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_fr_mx_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_de_in_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n",
            "train_all_equal_ca_us_full_aug_ftgt1_fsrc1\n",
            "\t['base', 'maml', 'forec']\n"
          ]
        }
      ],
      "source": [
        "command_dict = {}\n",
        "for tgt_market, src_market in all_poss_pairs:\n",
        "    for tgt_frac, src_fra in fractions:\n",
        "        cur_cmd_dict = {}\n",
        "        cur_exp_name = f'{exp_name_com}_{tgt_market}_{src_market}_{data_augment_method}_ftgt{tgt_frac}_fsrc{src_fra}'\n",
        "\n",
        "        # 'train_base.py'\n",
        "        py_file_main = 'train_base.py'\n",
        "        cur_exp_out_file = f'{exp_output_dir}base-{cur_exp_name}.json'\n",
        "        pre_set_args = {\n",
        "            \"--data_dir %s\"%(cur_data_path),\n",
        "            \"--tgt_market %s\"%(tgt_market),\n",
        "            \"--aug_src_market %s\"%(src_market),\n",
        "            \"--exp_name %s\"%(cur_exp_name),\n",
        "            \"--exp_output %s\"%(cur_exp_out_file),\n",
        "\n",
        "            \"--num_epoch %i\"%(25),\n",
        "            \"--batch_size %i\"%(1024),\n",
        "            \"--cuda \"\n",
        "\n",
        "            \"--data_augment_method %s\"%(data_augment_method),\n",
        "            \"--data_sampling_method %s\"%(sampling_method),\n",
        "\n",
        "            \"--tgt_fraction %i\"%(tgt_frac),\n",
        "            \"--src_fraction %i\"%(src_fra),\n",
        "        }\n",
        "        myargumets = ' '.join(pre_set_args)\n",
        "        command_pieces = [conda_python_dir, py_file_main, myargumets]\n",
        "        final_cmd = ' '.join(command_pieces)\n",
        "        cur_cmd_dict['base'] = final_cmd\n",
        "\n",
        "        if data_augment_method=='no_aug':\n",
        "            command_dict[cur_exp_name] = cur_cmd_dict\n",
        "            continue\n",
        "\n",
        "        # 'train_maml.py'\n",
        "        py_file_main = 'train_maml.py'\n",
        "        fast_lr_tune = '0.1'\n",
        "        shots = 20 #512, 200, 100, 50, 20\n",
        "        cur_exp_out_file = f'{exp_output_dir}maml-{cur_exp_name}_shots{shots}.json'\n",
        "        pre_set_args = {\n",
        "            \"--data_dir %s\"%(cur_data_path),\n",
        "            \"--tgt_market %s\"%(tgt_market),\n",
        "            \"--aug_src_market %s\"%(src_market),\n",
        "            \"--exp_name %s\"%(cur_exp_name),\n",
        "            \"--exp_output %s\"%(cur_exp_out_file),\n",
        "\n",
        "            \"--num_epoch %i\"%(25),\n",
        "            \"--batch_size %i\"%(shots),\n",
        "            \"--cuda \"\n",
        "\n",
        "            \"--data_sampling_method %s\"%(sampling_method),\n",
        "            \"--fast_lr %s\"%(fast_lr_tune),\n",
        "            \"--tgt_fraction %i\"%(tgt_frac),\n",
        "            \"--src_fraction %i\"%(src_fra),\n",
        "        }\n",
        "        myargumets = ' '.join(pre_set_args)\n",
        "        command_pieces = [conda_python_dir, py_file_main, myargumets]\n",
        "        final_cmd = ' '.join(command_pieces)\n",
        "        cur_cmd_dict['maml'] = final_cmd\n",
        "\n",
        "        # 'train_forec.py'\n",
        "        py_file_main = 'train_forec.py'\n",
        "        cur_exp_out_file = f'{exp_output_dir}forec-{cur_exp_name}_shots{shots}.json'\n",
        "        pre_set_args = {\n",
        "            \"--data_dir %s\"%(cur_data_path),\n",
        "            \"--tgt_market %s\"%(tgt_market),\n",
        "            \"--aug_src_market %s\"%(src_market),\n",
        "            \"--exp_name %s\"%(cur_exp_name),\n",
        "            \"--exp_output %s\"%(cur_exp_out_file),\n",
        "\n",
        "            \"--num_epoch %i\"%(25),\n",
        "            \"--batch_size %i\"%(shots),\n",
        "            \"--cuda \"\n",
        "\n",
        "            \"--data_sampling_method %s\"%(sampling_method),\n",
        "            \"--fast_lr %s\"%(fast_lr_tune),\n",
        "            \"--tgt_fraction %i\"%(tgt_frac),\n",
        "            \"--src_fraction %i\"%(src_fra),\n",
        "        }\n",
        "        myargumets = ' '.join(pre_set_args)\n",
        "        command_pieces = [conda_python_dir, py_file_main, myargumets]\n",
        "        final_cmd = ' '.join(command_pieces)\n",
        "        cur_cmd_dict['forec'] = final_cmd\n",
        "\n",
        "        command_dict[cur_exp_name] = cur_cmd_dict\n",
        "\n",
        "print(f'Generated {len(command_dict)} experiments:')\n",
        "for k, v in command_dict.items():\n",
        "    print(f'{k}')\n",
        "    print(f'\\t{list(v.keys())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "silent-nicholas",
      "metadata": {
        "id": "silent-nicholas"
      },
      "source": [
        "## Write commands into .sh bash scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "encouraging-consumption",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "encouraging-consumption",
        "outputId": "ae1dc661-5580-49a5-d7ba-14bb168849b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_all_equal_uk_us_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_jp_in_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_fr_jp_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_mx_in_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_ca_in_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_uk_in_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_in_ca_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_fr_uk_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_uk_de_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_de_ca_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_jp_ca_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_fr_de_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_fr_us_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_mx_ca_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_de_mx_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_in_mx_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_jp_mx_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_uk_ca_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_in_jp_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_fr_in_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_de_jp_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_de_fr_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_in_fr_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_jp_fr_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_ca_mx_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_mx_jp_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_uk_mx_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_ca_jp_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_ca_de_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_de_uk_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_in_uk_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_mx_fr_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_uk_jp_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_jp_uk_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_ca_fr_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_uk_fr_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_in_de_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_de_us_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_in_us_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_fr_ca_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_mx_uk_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_jp_us_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_jp_de_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_ca_uk_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_mx_de_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_mx_us_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_fr_mx_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_de_in_full_aug_ftgt1_fsrc1 bash is created!\n",
            "train_all_equal_ca_us_full_aug_ftgt1_fsrc1 bash is created!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "sh_files = 'scripts/'\n",
        "sh_logs = os.path.join(sh_files,'logs')\n",
        "checkpoints_dir = 'checkpoints/'\n",
        "if not os.path.exists(sh_logs):\n",
        "    os.mkdir(sh_files)\n",
        "    os.mkdir(sh_logs)\n",
        "if not os.path.exists(exp_output_dir):\n",
        "    os.mkdir(exp_output_dir)\n",
        "if not os.path.exists(checkpoints_dir):\n",
        "    os.mkdir(checkpoints_dir)\n",
        "\n",
        "gpu_num = 1\n",
        "gpu_type = '1080ti-long' #'titanx-short', 'm40-short'\n",
        "\n",
        "master_file = open(os.path.join(sh_files,'master.sh'), 'w')\n",
        "\n",
        "for cur_exp_name, v in command_dict.items():\n",
        "\n",
        "    bash_file_name = f'{cur_exp_name}-run.sh'\n",
        "    bash_file = open(os.path.join(sh_files,bash_file_name), 'w')\n",
        "    cur_log_file = os.path.join('logs', f'{cur_exp_name}.out')\n",
        "\n",
        "    bash_file.write('#!/bin/sh'+'\\n')\n",
        "    bash_file.write('#SBATCH --partition=%s'%(gpu_type) + '\\n')\n",
        "    bash_file.write('#SBATCH --ntasks=%s'%(1) + '\\n')\n",
        "    bash_file.write('#SBATCH --gres=gpu:%s'%(str(gpu_num)) + '\\n')\n",
        "    bash_file.write('#SBATCH --mem=%iG'%(50*gpu_num) + '\\n')\n",
        "    bash_file.write('#SBATCH --output=%s'%(cur_log_file) + '\\n')\n",
        "\n",
        "    bash_file.write('\\ncd ..\\n')\n",
        "    if 'base' in v:\n",
        "        bash_file.write(v['base'] + '\\n\\n')\n",
        "    if 'maml' in v:\n",
        "        bash_file.write(v['maml'] + '\\n\\n')\n",
        "    if 'forec' in v:\n",
        "        bash_file.write(v['forec'] + '\\n\\n')\n",
        "\n",
        "    bash_file.close()\n",
        "    master_file.write(f'sbatch {bash_file_name}\\n')\n",
        "    print(cur_exp_name + ' bash is created!')\n",
        "\n",
        "master_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "solar-samuel",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "solar-samuel",
        "outputId": "9c5ed7f2-107d-4c5c-f3ff-c9e57cfede49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./master.sh: line 1: sbatch: command not found\n",
            "./master.sh: line 2: sbatch: command not found\n",
            "./master.sh: line 3: sbatch: command not found\n",
            "./master.sh: line 4: sbatch: command not found\n",
            "./master.sh: line 5: sbatch: command not found\n",
            "./master.sh: line 6: sbatch: command not found\n",
            "./master.sh: line 7: sbatch: command not found\n",
            "./master.sh: line 8: sbatch: command not found\n",
            "./master.sh: line 9: sbatch: command not found\n",
            "./master.sh: line 10: sbatch: command not found\n",
            "./master.sh: line 11: sbatch: command not found\n",
            "./master.sh: line 12: sbatch: command not found\n",
            "./master.sh: line 13: sbatch: command not found\n",
            "./master.sh: line 14: sbatch: command not found\n",
            "./master.sh: line 15: sbatch: command not found\n",
            "./master.sh: line 16: sbatch: command not found\n",
            "./master.sh: line 17: sbatch: command not found\n",
            "./master.sh: line 18: sbatch: command not found\n",
            "./master.sh: line 19: sbatch: command not found\n",
            "./master.sh: line 20: sbatch: command not found\n",
            "./master.sh: line 21: sbatch: command not found\n",
            "./master.sh: line 22: sbatch: command not found\n",
            "./master.sh: line 23: sbatch: command not found\n",
            "./master.sh: line 24: sbatch: command not found\n",
            "./master.sh: line 25: sbatch: command not found\n",
            "./master.sh: line 26: sbatch: command not found\n",
            "./master.sh: line 27: sbatch: command not found\n",
            "./master.sh: line 28: sbatch: command not found\n",
            "./master.sh: line 29: sbatch: command not found\n",
            "./master.sh: line 30: sbatch: command not found\n",
            "./master.sh: line 31: sbatch: command not found\n",
            "./master.sh: line 32: sbatch: command not found\n",
            "./master.sh: line 33: sbatch: command not found\n",
            "./master.sh: line 34: sbatch: command not found\n",
            "./master.sh: line 35: sbatch: command not found\n",
            "./master.sh: line 36: sbatch: command not found\n",
            "./master.sh: line 37: sbatch: command not found\n",
            "./master.sh: line 38: sbatch: command not found\n",
            "./master.sh: line 39: sbatch: command not found\n",
            "./master.sh: line 40: sbatch: command not found\n",
            "./master.sh: line 41: sbatch: command not found\n",
            "./master.sh: line 42: sbatch: command not found\n",
            "./master.sh: line 43: sbatch: command not found\n",
            "./master.sh: line 44: sbatch: command not found\n",
            "./master.sh: line 45: sbatch: command not found\n",
            "./master.sh: line 46: sbatch: command not found\n",
            "./master.sh: line 47: sbatch: command not found\n",
            "./master.sh: line 48: sbatch: command not found\n",
            "./master.sh: line 49: sbatch: command not found\n"
          ]
        }
      ],
      "source": [
        "!cd scripts/ && chmod +x *.sh && ./master.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "medium-webcam",
      "metadata": {
        "id": "medium-webcam",
        "outputId": "1a0e04c6-fe1b-4cb9-bcb7-b2225c218f80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
            "           8377841 1080ti-lo train_al    bonab  R    8:57:51      1 node120\n",
            "           8377856 1080ti-lo train_al    bonab  R    8:57:51      1 node117\n",
            "           8376528  m40-long     bash    bonab  R 2-00:42:20      1 node013\n"
          ]
        }
      ],
      "source": [
        "!squeue -u bonab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "collect-tucson",
      "metadata": {
        "id": "collect-tucson"
      },
      "outputs": [],
      "source": [
        "!scancel 8377816"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "quick-catholic",
      "metadata": {
        "id": "quick-catholic"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py37_transformer3_qbd",
      "language": "python",
      "name": "py37_transformer3_qbd"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}